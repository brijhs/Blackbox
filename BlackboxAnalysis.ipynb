{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Package imports \n",
    "\n",
    "import pandas as pd\n",
    "import pandasql\n",
    "from pandasql import sqldf \n",
    "from sqlalchemy import create_engine\n",
    "import datetime\n",
    "from doctest import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import matplotlib_inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes on what the queries do \n",
    "\n",
    "basic_query: tests functionality, just grabs the 100 most recent commits from master_events \n",
    "\n",
    "joined_table: compiles relevant informmation for time-to-fix \n",
    "    included in the table: master_events.{session_id, event_id, created_at}, \n",
    "                           compile_events.{id, success}\n",
    "                           compile_outputs.{is_error, message}\n",
    "\n",
    "versionLinking: expansion on joined_table that standardizes by bluej version \n",
    "    takes a ranodm sample of the overall dataset: 22 million sessions on file for BlueJ5.x, each with about 100 events each, so maximum sample size will be around 500,000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id bluej_version java_version\n",
      "0  53011643  5.0.0preview       11.0.2\n",
      "1  53011690  5.0.0preview       11.0.2\n",
      "2  53011950  5.0.0preview       11.0.2\n",
      "3  53013239  5.0.0preview       11.0.2\n",
      "4  53149021  5.0.0preview       11.0.2\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "#SSH connection and queries \n",
    "\n",
    "#packages and init connection\n",
    "\n",
    "engine = create_engine(\"mysql://whitebox:ivorycube@127.0.0.1/blackbox_production\")\n",
    "\n",
    "#Good for testing functionality \n",
    "basic_query='select * from master_events order by created_at DESC limit 100'\n",
    "\n",
    "#Table that joins the important info \n",
    "joined_table=\"\"\"select myTable.*, compile_outputs.is_error, compile_outputs.message \n",
    "from (select master_events.session_id, master_events.event_id, master_events.created_at, \n",
    "compile_events.id, compile_events.success\n",
    "\tfrom master_events\n",
    "    JOIN compile_events \n",
    "    ON master_events.event_id = compile_events.id \n",
    "order by created_at DESC\n",
    "limit 100) myTable\n",
    "LEFT JOIN compile_outputs \n",
    "on myTable.id=compile_outputs.compile_event_id\n",
    "order by session_id DESC, created_at \n",
    "limit 1000;\"\"\"\n",
    "\n",
    "#df = pd.read_sql_query(joined_table, con=engine)\n",
    "\n",
    "#print(df.head(5))\n",
    "version =\"\"\"\n",
    "select sessions.id, installation_details.bluej_version, \n",
    "installation_details.java_version from installation_details\n",
    "join sessions\n",
    "on installation_details.bluej_version >= 5\n",
    "and installation_details.id = sessions.installation_details_id \n",
    "order by id ASC\n",
    "limit 10000\n",
    "\"\"\"\n",
    "\n",
    "version_df=pd.read_sql_query(version, con = engine)\n",
    "\n",
    "print(version_df.head(5))\n",
    "\n",
    "print(version_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cannot find symbol - variable', 'cyclic inheritance', 'array dimension missing']\n"
     ]
    }
   ],
   "source": [
    "#errorParse\n",
    "#Takes a string error message and checks it against some categories \n",
    "#Returns 'uncategorized' if not able to find a match \n",
    "#Set up a dictionary where the key is the error message and the value is the parsed version \n",
    "\n",
    "#Read csv in \n",
    "csv = pd.read_csv(\"ErrorParse.csv\").head(65)\n",
    "errorDict = dict()\n",
    "\n",
    "for i in csv.itertuples(): \n",
    "    errorDict.update({i[2]: i[3]})\n",
    "diag_message = list(errorDict.keys())\n",
    "parsed_message = list(errorDict.values())\n",
    "\n",
    "#checks error against dictionary \n",
    "def errorParse(message):\n",
    "    if message is None: \n",
    "        return 'No message provided'\n",
    "    for diagnostic in diag_message:\n",
    "        if diagnostic in message: \n",
    "            return errorDict[diagnostic]\n",
    "    else:\n",
    "        return 'uncategorized'\n",
    "\n",
    "errorParse('cannot find symbol -   variable string')\n",
    "\n",
    "#Checking on the none issue \n",
    "print(diag_message[-3::])\n",
    "for i in diag_message: \n",
    "    if type(i) is None or type(errorDict[i]) is None: \n",
    "        print('uh oh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Penultimate index name function \n",
    "def indexfunct(df): \n",
    "    for i in df.index: \n",
    "        return int(i)\n",
    "\n",
    "def penultimate(df): \n",
    "    return df.shape[0] + indexfunct(df) - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timeSubtract\n",
    "#Takes two times (YYYY-MM-DD HH:MM:SS) and returns a time in seconds \n",
    "def timeSubtract(firstTime, secondTime): \n",
    "    \"\"\"\n",
    "    >>> timeSubtract('2023-07-04 11:55:38', '2023-07-04 11:55:36')\n",
    "    2.0\n",
    "    >>> timeSubtract('2023-07-05 00:00:01', '2023-07-04 23:59:55')\n",
    "    6.0\n",
    "    >>> timeSubtract('2023-07-06 00:00:05', '2023-07-04 23:59:55')\n",
    "    300.0\n",
    "    \"\"\"\n",
    "    firstTime = datetime.datetime.strptime(str(firstTime), '%Y-%m-%d %H:%M:%S')\n",
    "    secondTime = datetime.datetime.strptime(str(secondTime), '%Y-%m-%d %H:%M:%S')\n",
    "    final_time =  float((firstTime - secondTime).total_seconds())\n",
    "    if final_time > 300: \n",
    "        final_time = 300\n",
    "    return final_time\n",
    "\n",
    "from doctest import *\n",
    "testmod()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple test Cases\n",
    "\n",
    "#Basic case \n",
    "testCase1 = {'session_id': [91407772, 91407772, 91407772], 'event_id' : [5, 5, 51817131], 'created_at': \n",
    "             ['2023-05-15 10:31:27', '2023-05-15 10:31:27', '2023-05-15 10:31:28'], 'id' : [5, 5, 51817131], 'is_error': [1.0, 1.0, 'NaN'], \n",
    "             'message': ['cannot find symbol -   method getLabel()', 'cannot find symbol -   method getLabel()', 'NaN'], 'success': [0,0,1]}\n",
    "\n",
    "test_df1 = pd.DataFrame(testCase1)\n",
    "\n",
    "\n",
    "#If session ends without solution\n",
    "testCase2 = {'session_id': [91407772, 91407772, 91407774], 'event_id' : [5, 5, 51817131], 'created_at': \n",
    "             ['2023-05-15 10:31:27', '2023-05-15 10:31:27', '2023-05-15 10:31:28'], 'id' : [5, 5, 51817131], 'is_error': [1.0, 1.0, 'NaN'], \n",
    "             'message': ['cannot find symbol -   method getLabel()', 'cannot find symbol -   method getLabel()', 'NaN'], 'success': [0,0,1]}\n",
    "test_df2 = pd.DataFrame(testCase2)\n",
    "\n",
    "#If new error occurs without solution, then solved\n",
    "testCase3 = {'session_id': [91407772, 91407772, 91407772], 'event_id' : [5, 5, 51817131], 'created_at': \n",
    "             ['2023-05-15 10:31:27', '2023-05-15 10:31:27', '2023-05-15 10:31:28'], 'id' : [5, 5, 51817131], 'is_error': [1.0, 1.0, 'NaN'], \n",
    "             'message': ['cannot find symbol -   method getLabel()', '; expected', 'NaN'], 'success': [0,0,1]}\n",
    "test_df3 = pd.DataFrame(testCase3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TimeDiff function \n",
    "#Returns time difference\n",
    "\n",
    "def timeDiff(df, rowNum):\n",
    "    \"\"\"\n",
    "    >>> timeDiff(test_df1, 0)\n",
    "    ['unknown method', 1.0, 3]\n",
    "    >>> timeDiff(test_df2, 0)\n",
    "    ['unknown method', 'NULL', 2]\n",
    "    >>> timeDiff(test_df3, 0)\n",
    "    ['unknown method', 1.0, 3]\n",
    "    >>> \n",
    "    \"\"\"\n",
    "    #Neds to take the first index, not the one passed to the function \n",
    "    #ex: if we're on the 30th row of iteration with a frame that goes from 2 to 100, it would calculate that the second to last index is 128 not 98 \n",
    "    penultRow = penultimate(df)\n",
    "    while df['success'][rowNum]==1:\n",
    "        if rowNum == penultRow: \n",
    "            if df['success'][rowNum] == 1: \n",
    "                return rowNum\n",
    "        else: \n",
    "            rowNum += 1\n",
    "    iterRow=rowNum\n",
    "    initError=errorParse(df['message'][rowNum])\n",
    "    initTime=df['created_at'][rowNum]\n",
    "    initSession=df['session_id'][rowNum]    \n",
    "    \n",
    "     #iterates through and checks if each compile is succesful \n",
    "    while df['success'][iterRow]==0: \n",
    "        if iterRow == penultRow: \n",
    "            if df['session_id'][iterRow + 1]!=initSession:\n",
    "                return [initError, 'NULL', iterRow + 1]\n",
    "            elif df['success'][iterRow + 1] == 1:\n",
    "                fixTime=df['created_at'][iterRow + 1]\n",
    "                return [initError, timeSubtract(fixTime, initTime), iterRow + 2]\n",
    "            else: \n",
    "                return [initError, 'NULL', iterRow + 2]\n",
    "        elif df['session_id'][iterRow]!=initSession: \n",
    "            return [initError, 'NULL', iterRow]\n",
    "        else: \n",
    "            iterRow+=1\n",
    "    if df['session_id'][iterRow]!=initSession: \n",
    "            return [initError, 'NULL', iterRow]\n",
    "    \n",
    "    fixTime=df['created_at'][iterRow]\n",
    "    return [initError, timeSubtract(fixTime, initTime), iterRow + 1]\n",
    "\n",
    "\n",
    "\n",
    "from doctest import *\n",
    "#testmod() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recordVal\n",
    "#Takes timeDiff output, errorTime, of format [ErrorParse, timeDiff, iterRow]\n",
    "#Also takes ongoing dataframe, timeFix, and writes to it  \n",
    "#iterRow, in this case, is the new row number \n",
    "\n",
    "def recordVal(timeFix, errorTime): \n",
    "    \"\"\"\n",
    "    >>> timeFix={'Error':['Variable not found', 'Incorrect call'], 'Time to Fix': [158, 200]}\n",
    "    >>> recordVal(timeFix, ['Class not found', 158, 6])[1]\n",
    "    6\n",
    "    >>> recordVal(timeFix, ['Incorrect variable declaration', 1221, 8])[1]\n",
    "    8\n",
    "    >>> timeFixdf=pd.DataFrame(timeFix)\n",
    "    >>> print(timeFixdf['Time to Fix'][3])\n",
    "    1221\n",
    "    >>> timeFix={'Error':[], 'Time to Fix': []}\n",
    "    >>> recordVal(timeFix, ['Example Error', 1234, 6])[1]\n",
    "    6\n",
    "    >>> timeFixdf = pd.DataFrame(timeFix)\n",
    "    >>> print(timeFixdf['Error'][0])\n",
    "    Example Error\n",
    "    >>> timeFix={'Error':[], 'Time to Fix': []}\n",
    "    >>> a = recordVal(timeFix, ['Example Error', 1234, 6])[1]\n",
    "    >>> recordVal(timeFix, 16)\n",
    "    [{'Error': ['Example Error'], 'Time to Fix': [1234]}, 16]\n",
    "    \n",
    "        \"\"\"\n",
    "    if type(errorTime) is int: \n",
    "        return [timeFix, errorTime]\n",
    "    timeFix['Error'].append(errorTime[0])\n",
    "    timeFix['Time to Fix'].append(errorTime[1])\n",
    "    return [timeFix, errorTime[2]]\n",
    "\n",
    "from doctest import *\n",
    "#testmod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counts all the errors, not just the ones used for newError()\n",
    "errorcount = {'Error': [], 'Count':[]}\n",
    "parsed_set = set(parsed_message)\n",
    "parsed_message = list(parsed_set)\n",
    "individualdict = {}\n",
    "all_count_sum = 0\n",
    "\n",
    "def countAll(df): \n",
    "    for message in parsed_message: \n",
    "        individualdict[message] = 0\n",
    "    individualdict['uncategorized'] = 0\n",
    "    individualdict['No message provided'] = 0\n",
    "    for i in range(df.shape[0]):\n",
    "        if df['success'][i] == 0: \n",
    "            errormessage = errorParse(df['message'][i])\n",
    "            individualdict[errormessage] = individualdict[errormessage] + 1 \n",
    "        else: \n",
    "            i +=1 \n",
    "    for key in individualdict.keys(): \n",
    "        errorcount['Error'].append(key)\n",
    "        errorcount['Count'].append(individualdict[key])\n",
    "    errorcountdf = pd.DataFrame(errorcount)\n",
    "    return errorcountdf\n",
    "\n",
    "errorcountdf = countAll(df)\n",
    "countTable = errorcountdf.sort_values(by = 'Count', ascending = False)\n",
    "\n",
    "for i in range(countTable.shape[0]): \n",
    "    all_count_sum += countTable['Count'][i]\n",
    "print(all_count_sum)\n",
    "\n",
    "countTable = countTable.head(30)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Zipf Mandelbrot Distribution\n",
    "plt.scatter(countTable['Error'], countTable['Count'])\n",
    "plt.xlabel('30 Most Common Error Messages', weight = 'bold')\n",
    "plt.ylabel('Message Count', weight = 'bold')\n",
    "plt.yscale(\"log\")\n",
    "ax = plt.gca()\n",
    "ax.set_xticks([])\n",
    "plt.suptitle('Frequency Distribution of Errors', weight = 'bold', fontsize = 14)\n",
    "plt.title('Sample Size: 50 million commits', fontsize = 10)\n",
    "\n",
    "plt.grid(axis = 'y', linestyle = '--')\n",
    "plt.yticks([100000, 1000000, 10000000])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(countTable['Error'], countTable['Count'] / all_count_sum)\n",
    "plt.xlabel('30 Most Common Error Messages', weight = 'bold')\n",
    "plt.ylabel('Frequency (Portion of Total Errors)', weight = 'bold')\n",
    "ax = plt.gca()\n",
    "ax.set_xticks([])\n",
    "plt.suptitle('Frequency Distribution of All Error Messages', weight = 'bold', fontsize = 14)\n",
    "plt.title('Sample Size: 50 million commits', fontsize = 10)\n",
    "\n",
    "plt.grid(axis = 'y', linestyle = '--')\n",
    "#plt.yticks([20000, 40000, 60000, 80000, 100000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#newError(): high-level iterator that calls all others \n",
    "timeFix = {'Error': [], 'Time to Fix': []}\n",
    "\n",
    "def newError(timeFix, df):\n",
    "    penultRow = penultimate(df)\n",
    "    i = indexfunct(df)\n",
    "    while True: \n",
    "        #If greater, return the runningDict and cal it a day \n",
    "        if i >= penultRow: \n",
    "            timeFixdf = pd.DataFrame(runningDict)\n",
    "            return timeFixdf \n",
    "        else: \n",
    "            a = recordVal(timeFix, timeDiff(df, i))\n",
    "            i = a[1]\n",
    "            runningDict = a[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling Processes\n",
    "Takes int sample_size, int repeat (how many times to sample), and the df you're searching\n",
    "Returns an errorsCalc esque dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sampling Function\n",
    "def singleSample(sample_size, df):\n",
    "    sample_df = df.sample(n=sample_size)\n",
    "    session_string = \"\"\n",
    "    for i in range(0, len(sample_df)):\n",
    "        session_string = session_string + \" master_events.session_id = {} OR\".format(str(sample_df['id'].iloc[i]))\n",
    "    \n",
    "    session_string = session_string + \" master_events.session_id = {}\".format(str(sample_df['id'].iloc[sample_size - 1]))\n",
    "\n",
    "    event_query = \"\"\"\n",
    "    select myTable.*, compile_outputs.message from (\n",
    "    select sessionTable.*, compile_events.id, compile_events.success from (\n",
    "    select master_events.session_id, master_events.event_id, master_events.created_at from master_events\n",
    "    where{})sessionTable\n",
    "    join compile_events \n",
    "    on sessionTable.event_id = compile_events.id)myTable\n",
    "    LEFT JOIN compile_outputs \n",
    "    on myTable.id=compile_outputs.compile_event_id\n",
    "    order by session_id DESC, created_at;\n",
    "    \"\"\".format(session_string)\n",
    "    \n",
    "    sampleEvents =pd.read_sql_query(event_query, con = engine)\n",
    "    print(\"Ratio of events to sessions: \" + str(sampleEvents.shape[0]/sample_size))\n",
    "    return sampleEvents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of events to sessions: 66.929\n",
      "Ratio of events to sessions: 74.059\n",
      "Ratio of events to sessions: 100.277\n",
      "Ratio of events to sessions: 87.525\n",
      "Ratio of events to sessions: 84.718\n",
      "Ratio of events to sessions: 74.113\n",
      "Ratio of events to sessions: 75.566\n",
      "Ratio of events to sessions: 76.414\n",
      "Ratio of events to sessions: 77.507\n",
      "Ratio of events to sessions: 90.43\n",
      "62082\n"
     ]
    }
   ],
   "source": [
    "#RunSample takes the sample and runs it through an errorsCalc with a set number of repeats\n",
    "#Need to check the values given by new Error here  \n",
    "\n",
    "def runSample(sampleSize, repeat, df): \n",
    "    final_errorsCalc = {'Error': [], 'Time to Fix': []}\n",
    "    i = 0\n",
    "    while i < repeat: \n",
    "        i +=1 \n",
    "        newError(final_errorsCalc, singleSample(sampleSize, df))\n",
    "    return pd.DataFrame(final_errorsCalc)\n",
    "\n",
    "testSample1 = runSample(1000, 10, version_df)\n",
    "\n",
    "print(testSample1.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Resolution Rate: \n",
      "93.74187224602109\n"
     ]
    }
   ],
   "source": [
    "#Unresolved vs. Resolved for each error\n",
    "df_string = 'testSample1'\n",
    "\n",
    "resolution_dict = {'Error' : [], 'Resolved' : [], 'Unresolved': [], 'Resolution Rate': []}\n",
    "for error in parsed_message:\n",
    "    resolved = 0\n",
    "    unresolved = 0\n",
    "    see_all = \"select {}.'Time to Fix' from {} where Error = '\".format(df_string, df_string) + str(error) + \"'\"\n",
    "    alldf = sqldf(see_all, locals())\n",
    "    for i in range(alldf.shape[0]):\n",
    "        if alldf['Time to Fix'][i] == 'NULL': \n",
    "            unresolved += 1\n",
    "        else: \n",
    "            resolved += 1\n",
    "    if unresolved == 0 and resolved ==0: \n",
    "        continue\n",
    "    resolution_dict['Error'].append(error)\n",
    "    resolution_dict['Resolved'].append(resolved)\n",
    "    resolution_dict['Unresolved'].append(unresolved)\n",
    "    if unresolved == 0 and resolved > 0: \n",
    "        res_rate = 100\n",
    "    else: \n",
    "        res_rate = resolved / (resolved + unresolved) * 100 \n",
    "    resolution_dict['Resolution Rate'].append(res_rate)\n",
    "\n",
    "resolutiondf = pd.DataFrame(resolution_dict)\n",
    "\n",
    "print('Average Resolution Rate: \\n' + str(resolutiondf['Resolution Rate'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#errorAnalysis, takes the dataframe of various instances of time and returns important stats by message \n",
    "#Note: takes df as a string of the name of the dataFrame\n",
    "\n",
    "def errorAnalysis(df): \n",
    "    analysisDict = {\"Error\" : [], \"Count\" : [], 'Median Time to Fix' : [],\n",
    "\"Average Time to Fix\": [],\n",
    "\"Maximum Time to Fix\": [],\n",
    "'Minimum Time to Fix': []} \n",
    "    parsed_set = set(parsed_message)\n",
    "    for error in parsed_set: \n",
    "        analysisDict['Error'].append(str(error))\n",
    "        all = {'Error': [], 'Time to Fix' : []}\n",
    "        noNull = {'Error': [], 'Time to Fix' : []}\n",
    "        for i in range(0, df.shape[0]):\n",
    "            if df['Error'].iloc[i] == error: \n",
    "                if df['Time to Fix'].iloc[i] != 'NULL':\n",
    "                    noNull['Error'].append(error)\n",
    "                    noNull['Time to Fix'].append(float(df['Time to Fix'].iloc[i]))\n",
    "                    all['Error'].append(error)\n",
    "                    all['Time to Fix'].append(float(df['Time to Fix'].iloc[i]))\n",
    "                all['Error'].append(error)\n",
    "                all['Time to Fix'].append(str(df['Time to Fix'].iloc[i]))\n",
    "        alldf = pd.DataFrame(all)\n",
    "        noNulldf = pd.DataFrame(noNull)\n",
    "        #Get the count \n",
    "        analysisDict['Count'].append(alldf.shape[0])\n",
    "        #Get the average \n",
    "        analysisDict['Average Time to Fix'].append(noNulldf['Time to Fix'].mean())\n",
    "        #Get the max\n",
    "        analysisDict['Maximum Time to Fix'].append(noNulldf['Time to Fix'].max())\n",
    "        #Get the min \n",
    "        analysisDict['Minimum Time to Fix'].append(noNulldf['Time to Fix'].min())\n",
    "        #Get the median \n",
    "        analysisDict['Median Time to Fix'].append(noNulldf['Time to Fix'].median())\n",
    "    return pd.DataFrame(analysisDict)\n",
    "\n",
    "\n",
    "parsed_set = set(parsed_message)\n",
    "parsed_message = list(parsed_set)\n",
    "sumStats = errorAnalysis(testSample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_set = set(parsed_message)\n",
    "for error in parsed_set: \n",
    "    noNull = {'Error': [], 'Time to Fix' : []}\n",
    "    for i in range(0, testSample1.shape[0]):\n",
    "        if testSample1['Error'].iloc[i] == error: \n",
    "            if testSample1['Time to Fix'].iloc[i] != 'NULL':\n",
    "                noNull['Error'].append(error)\n",
    "                noNull['Time to Fix'].append(float(testSample1['Time to Fix'].iloc[i]))\n",
    "noNulldf = pd.DataFrame(noNull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculates accuracy of parsing \n",
    "#Takes joinedTable dataframe as df\n",
    "#Parses each row and writes to a new table, which it will return \n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.options.display.max_rows = 1000\n",
    "\n",
    "\n",
    "def parseAccuracy(df): \n",
    "    parsedDf = {'Message': [], 'Parsed_Message': []}\n",
    "    for i in df.index: \n",
    "        #Only necessary when testing this function on its own \n",
    "        if df['is_error'][i] == 1: \n",
    "            message=str(df['message'][i])\n",
    "            parsedDf['Message'].append(message)\n",
    "            parsedDf['Parsed_Message'].append(errorParse(message))\n",
    "    return pd.DataFrame(parsedDf)\n",
    "\n",
    "x = parseAccuracy(df)\n",
    "other = \"SELECT * FROM x WHERE Parsed_Message='uncategorized' \"\n",
    "resolved = \" SELECT * FROM x WHERE Parsed_Message<>'uncategorized' \"\n",
    "unresolvedDf = sqldf(other, locals())\n",
    "resolvedDf = sqldf(resolved, locals())\n",
    "\n",
    "errorCount = df['success'].value_counts()\n",
    "errorCount = int(errorCount[[0][0]])\n",
    "len_unresolved = int(unresolvedDf.shape[0])\n",
    "unresolvedRatio = len_unresolved / errorCount\n",
    "accuracyRate = 1 - unresolvedRatio\n",
    "\n",
    "print(\"Total errors: \\n\" + str(errorCount))\n",
    "\n",
    "print('Total unresolved rows: \\n' + str(len_unresolved))\n",
    "\n",
    "print('Ratio of unresolved to total errors:\\n' + str(unresolvedRatio))\n",
    "\n",
    "print('Accuracy rate: \\n' + str(accuracyRate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort error messages by count, severity and show distribution\n",
    " \n",
    "table1 = sumStats.sort_values(by = 'Average Time to Fix', ascending = False)\n",
    "table2 = sumStats.sort_values(by = 'Count', ascending = False)\n",
    "mediantable = sumStats.sort_values(by='Median Time to Fix', ascending = False)\n",
    "\n",
    "print(\"The 5 Hardest errors sorted according to Median: \")\n",
    "print(mediantable.head(5))\n",
    "print('\\n')\n",
    "\n",
    "print(\"The 5 Hardest Errors:\")\n",
    "print(table1.head(5))\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "print(\"The 5 Easiest errors sorted according to Median: \")\n",
    "print(mediantable.tail(8))\n",
    "print('\\n')\n",
    "\n",
    "print(\"The 5 Easiest Errors:\")\n",
    "print(table1.tail(8))\n",
    "print('\\n')\n",
    "\n",
    "print(\"The errors sorted in order of frequency: \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram of median time to fix\n",
    "print('Median time to fix: ')\n",
    "print(errorsCalc['Time to Fix'].median())\n",
    "print(errorsCalc.shape[0])\n",
    "\n",
    "plt.hist(errorsCalc['Time to Fix'])\n",
    "plt.title(\"Histogram of Time to Fix Distribution \\n\", weight = 'bold', fontsize = 14)\n",
    "plt.xlabel(\"\\n Time to Fix (in seconds)\", weight = 'bold', fontsize = 10)\n",
    "plt.ylabel('Frequency (in millions of commits) \\n', weight = 'bold', fontsize = 10)\n",
    "plt.xticks([0,60,120,180,240,300])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Error severity \n",
    "severity = list()\n",
    "count_sum = 0\n",
    "\n",
    "for i in range(sumStats.shape[0]): \n",
    "    count_sum += float(sumStats['Count'][i])\n",
    "\n",
    "for i in range(sumStats.shape[0]):\n",
    "    s_index = float(float(sumStats['Count'][i]) / count_sum) * float(sumStats['Average Time to Fix'][i])\n",
    "    severity.append(s_index)\n",
    "\n",
    "sumStats['Severity Index'] = severity \n",
    "\n",
    "severity_sort = sumStats.sort_values(by = 'Severity Index', ascending = False)\n",
    "\n",
    "ps_df = severity_sort.head(5)\n",
    "ps_df.drop('Median Time to Fix', inplace = True, axis = 1)\n",
    "ps_df.drop('Maximum Time to Fix', inplace = True, axis = 1)\n",
    "ps_df.drop('Minimum Time to Fix', inplace = True, axis = 1)\n",
    "\n",
    "print('Severity Table: ')\n",
    "print(ps_df)\n",
    "ps_df.to_csv('out.csv')\n",
    "\n",
    "severity_sort.tail(5).to_csv('LeastSevere.csv')\n",
    "\n",
    "\n",
    "print('5 Hardest Errors by Severity: ')\n",
    "print(severity_sort.head(5))\n",
    "\n",
    "print('5 Easiest Errors by Severity: ')\n",
    "print(severity_sort.tail(8))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AI Comparison \n",
    "\n",
    "GPTdf = pd.read_csv('GPT_Fixes.csv').head(100)\n",
    "#May have to adjust for titles, but I don't think so \n",
    "\n",
    "#Of the form GPTdf = {'Filename' : [], 'Error Message' : [], \n",
    "#'Has Fix' : [], 'Fix Quality' : [], 'Fix Correct': [], 'Has Explanation' : [], \n",
    "#'Explanation Correct': []}\n",
    "\n",
    "#Want to find: GPT-4 resolution rate (rate of value 2 in Fix Correct)\n",
    "AIdict = {}\n",
    "totaldict = {}\n",
    "tempdf = severity_sort.head(5)\n",
    "top_five = tempdf['Error'].tolist()\n",
    "print(top_five)\n",
    "\n",
    "#Define top five as a string list of the five most severe errors\n",
    "for message in top_five: \n",
    "    AIdict[message] = 0\n",
    "    totaldict[message] = 0\n",
    "for i in range(GPTdf.shape[0]): \n",
    "    totaldict[GPTdf['Error Message'][i]] += 1\n",
    "    if GPTdf['Fix Correct'][i] == 2:\n",
    "        message_given = GPTdf['Error Message'][i]\n",
    "        AIdict[message_given] += 1\n",
    "\n",
    "AI_resrate = {'Message': [], 'Resolution Rate': [], 'Resolutions' : [], 'Total': []}\n",
    "\n",
    "for message in top_five:\n",
    "    AI_resrate['Message'].append(message)\n",
    "    res = AIdict[message]\n",
    "    tot = totaldict[message]\n",
    "    AI_resrate['Resolution Rate'].append(res / tot)\n",
    "    AI_resrate['Resolutions'].append(res)\n",
    "    AI_resrate['Total'].append(tot)\n",
    "\n",
    "AI_resratedf = pd.DataFrame(AI_resrate)\n",
    "print(AI_resratedf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Error percentage of top 5 (helfpul for report conclusion)\n",
    "\n",
    "#total errors: errorCount\n",
    "sum = 0\n",
    "for error in top_five: \n",
    "    sum += individualdict[error]\n",
    "\n",
    "print(sum/errorCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AI vs. human fix rate pair bar chart\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [7.50, 5.00]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "\n",
    "print(top_five)\n",
    "X = ['unknown \\n variable', 'semicolon \\n expected', 'reached end of file \\n while parsing', 'unknown \\n method', 'illegal start \\n of expression']\n",
    "AI_resrate = AI_resratedf['Resolution Rate'].tolist()\n",
    "Human_resrate = list()\n",
    "for message in top_five: \n",
    "    Human_df = (resolutiondf[resolutiondf['Error'] == message])\n",
    "    Human_resrate.append(Human_df['Resolution Rate'].tolist()[0] / 100)\n",
    "\n",
    "X_axis = np.arange(len(X))\n",
    "  \n",
    "plt.bar(X_axis - 0.2, AI_resrate, 0.4, label = 'GPT-4 Fix Rate')\n",
    "plt.bar(X_axis + 0.2, Human_resrate, 0.4, label = 'Human Fix Rate')\n",
    "\n",
    "spacing = 0.100\n",
    "plt.xticks(X_axis, X)\n",
    "plt.xlabel(\"Type of Error\", weight = 'bold', fontsize = 11)\n",
    "plt.ylabel(\"Rate of Succesful Compilation \\n Following Error\", weight = 'bold', fontsize = 11)\n",
    "plt.yticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2])\n",
    "plt.suptitle(\"Fix Rate of GPT-4 Versus Human Programmers\", weight = 'bold', fontsize = 14)\n",
    "plt.title('Sample Size: 500 million', fontsize = 10)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
