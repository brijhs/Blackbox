{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abstraction of the way it could work:\n",
    "\n",
    "#errorIter(): \n",
    "#iterate over each error \n",
    "#if new_error: \n",
    "    #call function newError():\n",
    "        #new function timeDiff()\n",
    "            #takes a row number as paramater \n",
    "            #calls errorParse()\n",
    "                #parse error into one of the discrete categories (still have to define the approach on this one)\n",
    "            #record initial created_at (first instance of this error)\n",
    "            #iterate through rows until succesful compilation/new error/bluej_finish\n",
    "            #take final crated_at (fix time or give up), substract from first instance\n",
    "            #return the difference  \n",
    "            \n",
    "        #return timeDiff, error name \n",
    "\n",
    "#new function recordTime()\n",
    "    #take timeDiff() time output and errorParse() and add them both to a table of instances \n",
    "\n",
    "#re-call newError() with (how to decide which row to work with)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Package imports \n",
    "\n",
    "import pandas as pd\n",
    "import pandasql\n",
    "from pandasql import sqldf \n",
    "from sqlalchemy import create_engine\n",
    "import datetime\n",
    "from doctest import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import matplotlib_inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes on what the queries do \n",
    "\n",
    "basic_query: tests functionality, just grabs the 100 most recent commits from master_events \n",
    "\n",
    "joined_table: compiles relevant informmation for time-to-fix \n",
    "    included in the table: master_events.{session_id, event_id, created_at}, \n",
    "                           compile_events.{id, success}\n",
    "                           compile_outputs.{is_error, message}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SSH connection and queries \n",
    "\n",
    "#packages and init connection\n",
    "\n",
    "engine = create_engine(\"mysql://whitebox:ivorycube@127.0.0.1/blackbox_production\")\n",
    "\n",
    "#Good for testing functionality \n",
    "basic_query='select * from master_events order by created_at DESC limit 100'\n",
    "\n",
    "#Table that joins the important info \n",
    "joined_table=\"\"\"select myTable.*, compile_outputs.is_error, compile_outputs.message \n",
    "from (select master_events.session_id, master_events.event_id, master_events.created_at, \n",
    "compile_events.id, compile_events.success\n",
    "\tfrom master_events\n",
    "    JOIN compile_events \n",
    "    ON master_events.event_id = compile_events.id \n",
    "order by created_at DESC\n",
    "limit 100000000) myTable\n",
    "LEFT JOIN compile_outputs \n",
    "on myTable.id=compile_outputs.compile_event_id\n",
    "order by session_id DESC, created_at \n",
    "limit 100000000;\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(joined_table, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#errorParse\n",
    "#Takes a string error message and checks it against some categories \n",
    "#Returns 'uncategorized' if not able to find a match \n",
    "#Set up a dictionary where the key is the error message and the value is the parsed version \n",
    "\n",
    "#Read csv in \n",
    "csv = pd.read_csv(\"ErrorParse.csv\").head(65)\n",
    "errorDict = dict()\n",
    "\n",
    "for i in csv.itertuples(): \n",
    "    errorDict.update({i[2]: i[3]})\n",
    "diag_message = list(errorDict.keys())\n",
    "parsed_message = list(errorDict.values())\n",
    "\n",
    "#checks error against dictionary \n",
    "def errorParse(message):\n",
    "    if message is None: \n",
    "        return 'No message provided'\n",
    "    for diagnostic in diag_message:\n",
    "        if diagnostic in message: \n",
    "            return errorDict[diagnostic]\n",
    "    else:\n",
    "        return 'uncategorized'\n",
    "\n",
    "errorParse('cannot find symbol -   variable string')\n",
    "\n",
    "#Checking on the none issue \n",
    "print(diag_message[-3::])\n",
    "for i in diag_message: \n",
    "    if type(i) is None or type(errorDict[i]) is None: \n",
    "        print('uh oh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Penultimate index name function \n",
    "def indexfunct(df): \n",
    "    for i in df.index: \n",
    "        return int(i)\n",
    "\n",
    "def penultimate(df): \n",
    "    return df.shape[0] + indexfunct(df) - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timeSubtract\n",
    "#Takes two times (YYYY-MM-DD HH:MM:SS) and returns a time in seconds \n",
    "def timeSubtract(firstTime, secondTime): \n",
    "    \"\"\"\n",
    "    >>> timeSubtract('2023-07-04 11:55:38', '2023-07-04 11:55:36')\n",
    "    2.0\n",
    "    >>> timeSubtract('2023-07-05 00:00:01', '2023-07-04 23:59:55')\n",
    "    6.0\n",
    "    >>> timeSubtract('2023-07-06 00:00:05', '2023-07-04 23:59:55')\n",
    "    86410.0\n",
    "    \"\"\"\n",
    "    firstTime = datetime.datetime.strptime(str(firstTime), '%Y-%m-%d %H:%M:%S')\n",
    "    secondTime = datetime.datetime.strptime(str(secondTime), '%Y-%m-%d %H:%M:%S')\n",
    "    final_time =  float((firstTime - secondTime).total_seconds())\n",
    "    if final_time > 300: \n",
    "        final_time = 300\n",
    "    return final_time\n",
    "\n",
    "from doctest import *\n",
    "#testmod()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TimeDiff function \n",
    "#Returns time difference\n",
    "\n",
    "def timeDiff(df, rowNum):\n",
    "    \"\"\"\n",
    "    >>> timeDiff(backup, 18)\n",
    "    ['unknown variable', 8.0, 26]\n",
    "    >>> timeDiff(session_end, 105)\n",
    "    ['possible uninitialized object', 'NULL', 110]\n",
    "    >>> timeDiff(list_135, 127)\n",
    "    ['class expected', 3.0, 135]\n",
    "    \"\"\"\n",
    "    #Neds to take the first index, not the one passed to the function \n",
    "    #ex: if we're on the 30th row of iteration with a frame that goes from 2 to 100, it would calculate that the second to last index is 128 not 98 \n",
    "    penultRow = penultimate(df)\n",
    "    while df['success'][rowNum]==1:\n",
    "        if rowNum == penultRow: \n",
    "            if df['success'][rowNum] == 1: \n",
    "                return rowNum\n",
    "        else: \n",
    "            rowNum += 1\n",
    "    iterRow=rowNum\n",
    "    initError=errorParse(df['message'][rowNum])\n",
    "    initTime=df['created_at'][rowNum]\n",
    "    initSession=df['session_id'][rowNum]    \n",
    "    \n",
    "     #iterates through and checks if each compile is succesful \n",
    "    while df['success'][iterRow]==0: \n",
    "        if iterRow == penultRow: \n",
    "            if df['session_id'][iterRow + 1]!=initSession:\n",
    "                return [initError, 'NULL', iterRow + 1]\n",
    "            elif df['success'][iterRow + 1] == 1: \n",
    "                fixTime=df['created_at'][iterRow + 1]\n",
    "                return [initError, timeSubtract(fixTime, initTime), iterRow + 2]\n",
    "            else: \n",
    "                return [initError, 'NULL', iterRow + 2]\n",
    "        elif df['session_id'][iterRow]!=initSession: \n",
    "            return [initError, 'NULL', iterRow]\n",
    "        else: \n",
    "            iterRow+=1\n",
    "    if df['session_id'][iterRow]!=initSession: \n",
    "            return [initError, 'NULL', iterRow]\n",
    "    fixTime=df['created_at'][iterRow]\n",
    "    return [initError, timeSubtract(fixTime, initTime), iterRow + 1]\n",
    "\n",
    "\n",
    "\n",
    "from doctest import *\n",
    "#testmod() \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recordVal\n",
    "#Takes timeDiff output, errorTime, of format [ErrorParse, timeDiff, iterRow]\n",
    "#Also takes ongoing dataframe, timeFix, and writes to it  \n",
    "#iterRow, in this case, is the new row number \n",
    "\n",
    "#Should this go after the iteration is done? \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def recordVal(timeFix, errorTime): \n",
    "    \"\"\"\n",
    "    >>> timeFix={'Error':['Variable not found', 'Incorrect call'], 'Time to Fix': [158, 200]}\n",
    "    >>> recordVal(timeFix, ['Class not found', 158, 6])[1]\n",
    "    6\n",
    "    >>> recordVal(timeFix, ['Incorrect variable declaration', 1221, 8])[1]\n",
    "    8\n",
    "    >>> timeFixdf=pd.DataFrame(timeFix)\n",
    "    >>> print(timeFixdf['Time to Fix'][3])\n",
    "    1221\n",
    "    >>> timeFix={'Error':[], 'Time to Fix': []}\n",
    "    >>> recordVal(timeFix, ['Example Error', 1234, 6])[1]\n",
    "    6\n",
    "    >>> timeFixdf = pd.DataFrame(timeFix)\n",
    "    >>> print(timeFixdf['Error'][0])\n",
    "    Example Error\n",
    "    >>> timeFix={'Error':[], 'Time to Fix': []}\n",
    "    >>> a = recordVal(timeFix, ['Example Error', 1234, 6])[1]\n",
    "    >>> recordVal(timeFix, 16)\n",
    "    [{'Error': ['Example Error'], 'Time to Fix': [1234]}, 16]\n",
    "    \n",
    "        \"\"\"\n",
    "    if type(errorTime) is int: \n",
    "        return [timeFix, errorTime]\n",
    "    timeFix['Error'].append(errorTime[0])\n",
    "    timeFix['Time to Fix'].append(errorTime[1])\n",
    "    return [timeFix, errorTime[2]]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "timeFix={'Error':[], 'Time to Fix': []}\n",
    "print(recordVal(timeFix, ['Variable not found', 155, 8]))\n",
    "print(recordVal(timeFix, ['Incorrect call', 'NULL', 17]))\n",
    "timeFixdf = pd.DataFrame(timeFix)\n",
    "print(timeFixdf)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from doctest import *\n",
    "#testmod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counts all the errors, not just the ones used for newError()\n",
    "errorcount = {'Error': [], 'Count':[]}\n",
    "parsed_set = set(parsed_message)\n",
    "parsed_message = list(parsed_set)\n",
    "individualdict = {}\n",
    "all_count_sum = 0\n",
    "\n",
    "def countAll(df): \n",
    "    for message in parsed_message: \n",
    "        individualdict[message] = 0\n",
    "    individualdict['uncategorized'] = 0\n",
    "    individualdict['No message provided'] = 0\n",
    "    for i in range(df.shape[0]):\n",
    "        if df['success'][i] == 0: \n",
    "            errormessage = errorParse(df['message'][i])\n",
    "            individualdict[errormessage] = individualdict[errormessage] + 1 \n",
    "        else: \n",
    "            i +=1 \n",
    "    for key in individualdict.keys(): \n",
    "        errorcount['Error'].append(key)\n",
    "        errorcount['Count'].append(individualdict[key])\n",
    "    errorcountdf = pd.DataFrame(errorcount)\n",
    "    return errorcountdf\n",
    "\n",
    "errorcountdf = countAll(df)\n",
    "countTable = errorcountdf.sort_values(by = 'Count', ascending = False)\n",
    "\n",
    "for i in range(countTable.shape[0]): \n",
    "    all_count_sum += countTable['Count'][i]\n",
    "print(all_count_sum)\n",
    "\n",
    "countTable = countTable.head(30)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(countTable['Error'], countTable['Count'])\n",
    "plt.xlabel('30 Most Common Error Messages', weight = 'bold')\n",
    "plt.ylabel('Message Count', weight = 'bold')\n",
    "plt.yscale(\"log\")\n",
    "ax = plt.gca()\n",
    "ax.set_xticks([])\n",
    "plt.suptitle('Frequency Distribution of Errors', weight = 'bold', fontsize = 14)\n",
    "plt.title('Sample Size: 50 million commits', fontsize = 10)\n",
    "\n",
    "plt.grid(axis = 'y', linestyle = '--')\n",
    "plt.yticks([100000, 1000000, 10000000])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(countTable['Error'], countTable['Count'] / all_count_sum)\n",
    "plt.xlabel('30 Most Common Error Messages', weight = 'bold')\n",
    "plt.ylabel('Frequency (Portion of Total Errors)', weight = 'bold')\n",
    "ax = plt.gca()\n",
    "ax.set_xticks([])\n",
    "plt.suptitle('Frequency Distribution of All Error Messages', weight = 'bold', fontsize = 14)\n",
    "plt.title('Sample Size: 50 million commits', fontsize = 10)\n",
    "\n",
    "plt.grid(axis = 'y', linestyle = '--')\n",
    "#plt.yticks([20000, 40000, 60000, 80000, 100000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#newError(): high-level iterator that calls all others \n",
    "timeFix = {'Error': [], 'Time to Fix': []}\n",
    "\n",
    "def newError(df):\n",
    "    penultRow = penultimate(df)\n",
    "    i = indexfunct(df)\n",
    "    while True: \n",
    "        #If greater, return the runningDict and cal it a day \n",
    "        if i >= penultRow: \n",
    "            timeFixdf = pd.DataFrame(runningDict)\n",
    "            return timeFixdf \n",
    "        else: \n",
    "            a = recordVal(timeFix, timeDiff(df, i))\n",
    "            i = a[1]\n",
    "            runningDict = a[0] \n",
    "\n",
    "#print(newError(secondTestdf))\n",
    "#unknown method, first error, throwing -6.0 seconds fix time \n",
    "\n",
    "\n",
    "errorsCalc = newError(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(errorsCalc.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unresolved vs. Resolved for each error\n",
    "\n",
    "resolution_dict = {'Error' : [], 'Resolved' : [], 'Unresolved': [], 'Resolution Rate': []}\n",
    "\n",
    "for error in parsed_message:\n",
    "    resolved = 0\n",
    "    unresolved = 0\n",
    "    see_all = \"select errorsCalc.'Time to Fix' from errorsCalc where Error = '\" + str(error) + \"'\"\n",
    "    alldf = sqldf(see_all, locals())\n",
    "    for i in range(alldf.shape[0]):\n",
    "        if alldf['Time to Fix'][i] == 'NULL': \n",
    "            unresolved += 1\n",
    "        else: \n",
    "            resolved += 1\n",
    "    resolution_dict['Error'].append(error)\n",
    "    resolution_dict['Resolved'].append(resolved)\n",
    "    resolution_dict['Unresolved'].append(unresolved)\n",
    "    if unresolved == 0: \n",
    "        res_rate = 1\n",
    "    else: \n",
    "        res_rate = resolved / (resolved + unresolved) * 100 \n",
    "    resolution_dict['Resolution Rate'].append(res_rate)\n",
    "\n",
    "resolutiondf = pd.DataFrame(resolution_dict)\n",
    "\n",
    "print('Average Resolution Rate: \\n' + str(resolutiondf['Resolution Rate'].mean()))\n",
    "\n",
    "print(resolutiondf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set nulls = 3000 seconds \n",
    "TF_list = errorsCalc['Time to Fix'].tolist()\n",
    "new_list = list()\n",
    "\n",
    "for element in TF_list:\n",
    "    if element == 'NULL': \n",
    "        new_list.append(300)\n",
    "    else: \n",
    "        new_list.append(element)\n",
    "errorsCalc['Time to Fix'] = new_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average Time to Fix \n",
    "print(errorsCalc['Time to Fix'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#errorAnalysis, takes the dataframe of various instances of time and returns important stats by message \n",
    "analysisDict = {\"Error\" : [], \"Count\" : [], 'Median Time to Fix' : [],\n",
    "\"Average Time to Fix\": [],\n",
    "\"Maximum Time to Fix\": [],\n",
    "'Minimum Time to Fix': []} \n",
    "\n",
    "\n",
    "parsed_set = set(parsed_message)\n",
    "parsed_message = list(parsed_set)\n",
    "\n",
    "def errorAnalysis(errorsCalc): \n",
    "    for error in parsed_message: \n",
    "        analysisDict['Error'].append(str(error))\n",
    "        see_all = \"select errorsCalc.'Time to Fix' from errorsCalc where Error = '\" + str(error) + \"'\"\n",
    "        alldf = sqldf(see_all, locals())\n",
    "        noNulls = \" select errorsCalc.'Time to Fix' from errorsCalc where errorsCalc.'Time to Fix'<>'NULL' and Error = '\" + str(error) + \"'\"\n",
    "        noNulldf = sqldf(noNulls, locals()) \n",
    "        #Get the count \n",
    "        analysisDict['Count'].append(alldf.shape[0])\n",
    "        #Get the average \n",
    "        avg_query = \" select avg(alldf.'Time to Fix') from alldf\"\n",
    "        avgdf = sqldf(avg_query, locals())\n",
    "        analysisDict['Average Time to Fix'].append(avgdf.iloc[0][0])\n",
    "        #Get the max\n",
    "        analysisDict['Maximum Time to Fix'].append(noNulldf.max().iloc[0])\n",
    "        #Get the min \n",
    "        analysisDict['Minimum Time to Fix'].append(noNulldf.min().iloc[0])\n",
    "        #Get the median \n",
    "        analysisDict['Median Time to Fix'].append(noNulldf.median().iloc[0])\n",
    "    return pd.DataFrame(analysisDict)\n",
    "\n",
    "sumStats = errorAnalysis(errorsCalc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ParseAccuracy calculator \n",
    "#Takes joinedTable dataframe as df\n",
    "#Parses each row and writes to a new table, which it will return \n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.options.display.max_rows = 1000\n",
    "\n",
    "\n",
    "def parseAccuracy(df): \n",
    "    parsedDf = {'Message': [], 'Parsed_Message': []}\n",
    "    for i in df.index: \n",
    "        #Only necessary when testing this function on its own \n",
    "        if df['is_error'][i] == 1: \n",
    "            message=str(df['message'][i])\n",
    "            parsedDf['Message'].append(message)\n",
    "            parsedDf['Parsed_Message'].append(errorParse(message))\n",
    "    return pd.DataFrame(parsedDf)\n",
    "\n",
    "x = parseAccuracy(df)\n",
    "other = \"SELECT * FROM x WHERE Parsed_Message='uncategorized' \"\n",
    "resolved = \" SELECT * FROM x WHERE Parsed_Message<>'uncategorized' \"\n",
    "unresolvedDf = sqldf(other, locals())\n",
    "resolvedDf = sqldf(resolved, locals())\n",
    "\n",
    "errorCount = df['success'].value_counts()\n",
    "errorCount = int(errorCount[[0][0]])\n",
    "len_unresolved = int(unresolvedDf.shape[0])\n",
    "unresolvedRatio = len_unresolved / errorCount\n",
    "accuracyRate = 1 - unresolvedRatio\n",
    "\n",
    "print(\"Total errors: \\n\" + str(errorCount))\n",
    "\n",
    "print('Total unresolved rows: \\n' + str(len_unresolved))\n",
    "\n",
    "print('Ratio of unresolved to total errors:\\n' + str(unresolvedRatio))\n",
    "\n",
    "print('Accuracy rate: \\n' + str(accuracyRate))\n",
    "\n",
    "\n",
    "#print(unresolvedDf[['Message', 'Parsed_Message']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matplot fun \n",
    "#Gotta look into this more \n",
    "#Should import sumStats from ErrorAnalysis \n",
    "#Need to find: top 5 error messages in time to fix, bottom 5 in time to fix \n",
    "#sort by count and show the distribution \n",
    "\n",
    "#Sort by count and show distribution\n",
    " \n",
    "table1 = sumStats.sort_values(by = 'Average Time to Fix', ascending = False)\n",
    "table2 = sumStats.sort_values(by = 'Count', ascending = False)\n",
    "mediantable = sumStats.sort_values(by='Median Time to Fix', ascending = False)\n",
    "\n",
    "print(\"The 5 Hardest errors sorted according to Median: \")\n",
    "print(mediantable.head(5))\n",
    "print('\\n')\n",
    "\n",
    "print(\"The 5 Hardest Errors:\")\n",
    "print(table1.head(5))\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "print(\"The 5 Easiest errors sorted according to Median: \")\n",
    "print(mediantable.tail(8))\n",
    "print('\\n')\n",
    "\n",
    "print(\"The 5 Easiest Errors:\")\n",
    "print(table1.tail(8))\n",
    "print('\\n')\n",
    "\n",
    "print(\"The errors sorted in order of frequency: \\n\")\n",
    "\n",
    "\n",
    "#matplot graphs (maybe do this in R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(table1.head(1))\n",
    "print(table1.tail(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Median time to fix: ')\n",
    "print(errorsCalc['Time to Fix'].median())\n",
    "print(errorsCalc.shape[0])\n",
    "\n",
    "plt.hist(errorsCalc['Time to Fix'])\n",
    "plt.title(\"Histogram of Time to Fix Distribution \\n\", weight = 'bold', fontsize = 14)\n",
    "plt.xlabel(\"\\n Time to Fix (in seconds)\", weight = 'bold', fontsize = 10)\n",
    "plt.ylabel('Frequency (in millions of commits) \\n', weight = 'bold', fontsize = 10)\n",
    "plt.xticks([0,60,120,180,240,300])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#60-second analysis \n",
    "\n",
    "quick_fix = \"Select * from errorsCalc where errorsCalc.'Time to Fix' <= 60\"\n",
    "quickdf = sqldf(quick_fix, locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the histogram \n",
    "\n",
    "plt.hist(quickdf['Time to Fix'])\n",
    "plt.title(\"Histogram of Time to Fix Distribution \\n with Fixes Under One Minute\", weight = 'bold', fontsize = 14)\n",
    "plt.xlabel(\"\\n Time to Fix (in seconds)\", weight = 'bold', fontsize = 10)\n",
    "plt.ylabel('Frequency (in millions of commits) \\n', weight = 'bold', fontsize = 10)\n",
    "plt.xticks([0, 12, 24, 36, 48, 60])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Error severity \n",
    "severity = list()\n",
    "count_sum = 0\n",
    "\n",
    "for i in range(sumStats.shape[0]): \n",
    "    count_sum += float(sumStats['Count'][i])\n",
    "\n",
    "for i in range(sumStats.shape[0]):\n",
    "    s_index = float(float(sumStats['Count'][i]) / count_sum) * float(sumStats['Average Time to Fix'][i])\n",
    "    severity.append(s_index)\n",
    "\n",
    "sumStats['Severity Index'] = severity \n",
    "\n",
    "severity_sort = sumStats.sort_values(by = 'Severity Index', ascending = False)\n",
    "\n",
    "ps_df = severity_sort.head(5)\n",
    "ps_df.drop('Median Time to Fix', inplace = True, axis = 1)\n",
    "ps_df.drop('Maximum Time to Fix', inplace = True, axis = 1)\n",
    "ps_df.drop('Minimum Time to Fix', inplace = True, axis = 1)\n",
    "\n",
    "print('Severity Table: ')\n",
    "print(ps_df)\n",
    "ps_df.to_csv('out.csv')\n",
    "\n",
    "severity_sort.tail(5).to_csv('LeastSevere.csv')\n",
    "\n",
    "\n",
    "print('5 Hardest Errors by Severity: ')\n",
    "print(severity_sort.head(5))\n",
    "\n",
    "print('5 Easiest Errors by Severity: ')\n",
    "print(severity_sort.tail(8))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AI Comparison \n",
    "\n",
    "GPTdf = pd.read_csv('GPT_Fixes.csv').head(100)\n",
    "#May have to adjust for titles, but I don't think so \n",
    "\n",
    "#Probably will look like this GPTdf = {'Filename' : [], 'Error Message' : [], \n",
    "#'Has Fix' : [], 'Fix Quality' : [], 'Fix Correct': [], 'Has Explanation' : [], \n",
    "#'Explanation Correct': []}\n",
    "\n",
    "#Want to find: resolution rate (rate of 2 under Fix Correct)\n",
    "AIdict = {}\n",
    "totaldict = {}\n",
    "tempdf = severity_sort.head(5)\n",
    "top_five = tempdf['Error'].tolist()\n",
    "print(top_five)\n",
    "\n",
    "#Define top five as a string list of the five most severe errors\n",
    "for message in top_five: \n",
    "    AIdict[message] = 0\n",
    "    totaldict[message] = 0\n",
    "for i in range(GPTdf.shape[0]): \n",
    "    totaldict[GPTdf['Error Message'][i]] += 1\n",
    "    if GPTdf['Fix Correct'][i] == 2:\n",
    "        message_given = GPTdf['Error Message'][i]\n",
    "        AIdict[message_given] += 1\n",
    "\n",
    "AI_resrate = {'Message': [], 'Resolution Rate': [], 'Resolutions' : [], 'Total': []}\n",
    "\n",
    "for message in top_five:\n",
    "    AI_resrate['Message'].append(message)\n",
    "    res = AIdict[message]\n",
    "    tot = totaldict[message]\n",
    "    AI_resrate['Resolution Rate'].append(res / tot)\n",
    "    AI_resrate['Resolutions'].append(res)\n",
    "    AI_resrate['Total'].append(tot)\n",
    "\n",
    "AI_resratedf = pd.DataFrame(AI_resrate)\n",
    "print(AI_resratedf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Error percentage of top 5 \n",
    "\n",
    "#total errors: errorCount\n",
    "sum = 0\n",
    "for error in top_five: \n",
    "    sum += individualdict[error]\n",
    "\n",
    "print(sum/errorCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AI pair chart \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [7.50, 5.00]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "\n",
    "print(top_five)\n",
    "X = ['unknown \\n variable', 'semicolon \\n expected', 'reached end of file \\n while parsing', 'unknown \\n method', 'illegal start \\n of expression']\n",
    "AI_resrate = AI_resratedf['Resolution Rate'].tolist()\n",
    "Human_resrate = list()\n",
    "for message in top_five: \n",
    "    Human_df = (resolutiondf[resolutiondf['Error'] == message])\n",
    "    Human_resrate.append(Human_df['Resolution Rate'].tolist()[0] / 100)\n",
    "\n",
    "X_axis = np.arange(len(X))\n",
    "  \n",
    "plt.bar(X_axis - 0.2, AI_resrate, 0.4, label = 'GPT-4 Fix Rate')\n",
    "plt.bar(X_axis + 0.2, Human_resrate, 0.4, label = 'Human Fix Rate')\n",
    "\n",
    "spacing = 0.100\n",
    "plt.xticks(X_axis, X)\n",
    "plt.xlabel(\"Type of Error\", weight = 'bold', fontsize = 11)\n",
    "plt.ylabel(\"Rate of Succesful Compilation \\n Following Error\", weight = 'bold', fontsize = 11)\n",
    "plt.yticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2])\n",
    "plt.suptitle(\"Fix Rate of GPT-4 Versus Human Programmers\", weight = 'bold', fontsize = 14)\n",
    "plt.title('Sample Size: 500 million', fontsize = 10)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Steps to read local tables with SQL \n",
    "\n",
    "#Create the dataframe \n",
    "sample_DF = {'Error': ['Variable not found', 'Method call incorrect'], 'Number': [1, 2]}\n",
    "a = pd.DataFrame(sample_DF)\n",
    "print(a)\n",
    "\n",
    "#use the sqldf package with a predefined query\n",
    "query = \"SELECT * FROM a where Error LIKE '%Variable %' \"\n",
    "newDf = sqldf(query, locals())\n",
    "#locals() identifies where the table is stored \n",
    "\n",
    "print(newDf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
